\documentclass[a4paper, 12pt]{report}
\usepackage{amsmath, amsthm, amsfonts,amssymb} % depended by \DeclareMathOperator, \newtheorem, \proof, \mathbb
\usepackage{mathrsfs} % for mathscr
\usepackage[utf8]{inputenc} % utf8 encoding
\usepackage{listings} % for lst in color box
\usepackage[most]{tcolorbox} % for messageshell colorbox
\usepackage[subsection]{algorithm} % depended by algpseudocode
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{hyperref} % add clickable link to tableofcontents
\usepackage{enumitem} % use enumerate
\usepackage{float} % for [H] label in figures

\usepackage{fancyhdr} % add header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\leftmark}
\cfoot{\thepage}

\graphicspath{{../img/}}

\usepackage{geometry} % set margin
\geometry{left=3cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\title{A Tree Search/Constraint Propagation algorithm for the Sudoku Problem}
\date{}
\author{Shitong CHAI}

%\begin{figure}[H]
%\centering
%\includegraphics[width=0.8\textwidth,height=50mm]{}
%\caption{}
%\end{figure}

\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction}

    \section {Sudoku game}
        \subsection {Glossary of Sudoku}
            A puzzle of Sudoku is given by a grid of 81 squares, every square can take the digit value from 1 to 9. A grid is consisted of rows, columns and boxes, each row, column or box is consisted of 9 squares and these squares may have overlapping. Each row, column or box is called a unit. For a square, it is located on exactly 3 units, a row, a column and a box. All the squares in these 3 units(except itself), is the peers of the square. Every digit value taken by a square should by
            different from all its peers and the digit values taken by the squares of a unit should be all different.

        \subsection {Input Format}

            The grid of one Sudoku puzzle is represented by a string of 81 characters ended by a line break. Every character which is integer from 1 to 9 is an preassigned digit and a period represents an unknown square. An example is shown in figure \ref{examplesudoku}.
            \begin{figure}[H]
                \begin{lstlisting}[frame=single]
4.....8.5.3..........7......2.....6..... 8.4......1.......6.3.7.5..2.....1.4......
                \end{lstlisting}
                \label{examplesudoku}
                \caption{Example of input format}
            \end{figure}

            For C language, the input grids can be parsed by the function \emph{grid\_values} which is as follows.
            \begin{lstlisting}[frame=single, language=C]
void grid_values(char* grid, bool values[81][9])
{
    // assign values to the domain of squares according to a grid string
    for(int j=0; j<81; j++)
    {
            for(int k=0; k<9; k++)
            {
                values[j][k] = true;
            }
    }
    int idx=0, count=0;
    while(grid[idx]!='\0' && count<81)
    {
        if(grid[idx]>=49 && grid[idx]<=57)
        {
            if(!assign(values, count, grid[idx]-49))
            {
                printf("assigning failed!\n");
            }
            count++;
        }
        if(grid[idx]=='0' || grid[idx]=='.')
        {
            count++;
        }
        idx++;
    }
}
            \end{lstlisting}

            For the Python version, grids can be parsed with the following code.

            \begin{lstlisting}[frame=single, language=python]
digits   = '123456789'
def grid_values(grid):
    "Convert grid into a dict of {square: char} with '0' or '.' for empties."
    ints = [int(c) if c in digits else 0 for c in grid ]
    assert len(ints) == 81
    return {k:v for k,v in dict(list(zip(squares, ints))).items() if v!=0}
            \end{lstlisting}


    
    \section {Constraint Satisfaction Problems}

        \subsection {Definitions}
            Constraint Satisfaction Problem is a triple $\mathcal P=\langle X, D, C\rangle$, where $X$ is the tuple of all the variables $X=\langle x_1,x_2,\cdots,x_n\rangle$, every variable takes value from a domain $x_i\in D(x_i)=D_i$; $D$ is the tuple of all the domains $D=D(x_1)\times D(x_2)\times \cdots D(x_n)=\langle D_1,D_2,\cdots,D_n\rangle$; $C$ is the set of all the constraints $C=\{c_1,c_2,\cdots,c_t\}$ where a constraint $c_j$ is a pair $\langle c(X(c_j)),
            X(c_j)\rangle$, $X(c_j)$ is the scheme of constraint $c_j$ consisted of all the variables restricted by constraint $c_j$ and $c(V)$ represents the relation of variables in $V$. A CSP $N=\langle X,D,C\rangle$ can also be called a network.

            Given a tuple $d$ on a sequence $Y$ of variables, and given $W\subseteq Y$, $d[W]$ denotes the subset of tuple on the sequence $W$. Given $x_i\in Y$, $d[x_i]$ denotes the value of $x_i$ in $d$.

            For a CSP, a set of variables can be tentatively assigned values to study the consistency with constraints, so we have the definition of instantiation\cite{rossi2006handbook}.
            \begin{definition}
                (Instantiation) Given a network $N=\langle X,D,C\rangle$,

                An instantiation $I$ on $Y=\langle x_1,\cdots,x_k\rangle \subseteq X$ is an assignment of values $d_1,\cdots, d_k$ to the variables $x_1, \cdots, x_k$ i.e. $I$ is a tuple on $Y$. $I$ can be denoted by $\{x_1\leftarrow d_1, \cdots, x_k\leftarrow d_k\}=\langle d_1, \cdots, d_k\rangle$.

                An instantiation $I$ on $Y$ is valid if $\forall x_i\in Y, I[x_i]\in D(x_i)$.

                An instantiation $I$ on $Y$ is locally consistent iff it is valid for all $c\in C$ with $X(c)\subseteq Y$, $I[X(c)]$ satisfies $c$. If $I$ is not locally consistent, it is locally inconsistent.

                A solution $sol(N)$ to a network $N$ is an instantiation $I$ on $X$ which is locally consistent. 

                An instantiation $I$ on $Y$ is globally consistent if it can be extended to a solution backtrack-freely.
            \end{definition}

            Given a network, if it is globally consistent, then we can start with any locally consistent instantiation and get a solution backtrack-freely, which is what we want. However, most of the time the network given by a CSP is not globally consistent and it is hard to transform it to a globally consistent one. To distinguish between these networks by hardness of finding a solution backtrack freely, we need the definition of preorder $\preceq$ on networks.

            \begin{definition}
                (Preorder $\preceq$ on networks\cite{rossi2006handbook}) Given networks $N$ and $N'$, $N'\preceq N$ iff $X_{N'}=X_N$ and any instantiation $I$ on $Y\subseteq X_N$ locally inconsistent in $N$ is locally inconsistent in $N'$ as well.
            \end{definition}

            Networks which is precedent in this preorder is tighter, which means there are less solution or more constraints in the precedent ones.

            \begin{definition}
                (Tightenings of a network\cite{rossi2006handbook}) The space of $\mathcal P_N$ of all possible tightenings of a network $N=\langle X,D,C\rangle$ is the set of networks $N'=\langle X,D',C'\rangle$ such that $D'\subseteq D$ and for all $c\in C$ there exists $c'\in C'$ with $X(c')=X(c)$ and $c'\subset c$.
            \end{definition}

            Among all the tightening of a network, we are interested in the networks that preserves the solution to the original network, which means a global consistent network.

            \begin{definition}
                (Global consistency\cite{rossi2006handbook}) Let $N=\langle X,D,C\rangle$ be a network, and $N_G=\langle X,D_G,C_G\rangle$ be a network in $\mathcal P^{sol}_N$. If for all $N'\in \mathcal P^{sol}_N$, $N_G\preceq N'$, then any instantiation $I$ on $Y\subseteq X$ which is locally consistent in $N_G$ can be extended to a solution of $N$. $N_G$ is called a globally consistent network.
            \end{definition}

            However, it has been proven that finding a global consistency network in $\mathcal P^{sol}_N$ is as hard as searching with backtracking, which is NP. To make the network as tight as possible without losing any solution, we have to use a property which characterizes the local consistency of partial instantiations to reduce the search space. In other words, given a property $\Phi$, we want to find a domain based tightening which tightens the network by
            shrinking the size of its domain.

            For sudoku problem, the AllDifferent constraints can be decomposed into many binary constraints, so the network is both normalized and embedded, stable under union, so it suffice to only considering domain based tightening.


            For every CSP, it is relatively easier to use Hyper Arc Consistency(HAC) to perform tightening and constraint propagation other than finding a globally consistent network. HAC is defined as follows.
            \begin{definition}
                (Hyper Arc Consistency) Given a network $N=\langle X,D,C\rangle$, a constraint $c\in C$ and a variable $x_i\in X(c)$,

                A value $d_i\in D(x_i)$ is consistent with $c$ in $D$ iff there exists a valid tuple $d$ satisfing $c$ such that $d_i=d[x_i]$. Such a tuple is called a support for assigning $x_i\leftarrow d_i$ on $c$.

                The domain $D$ is hyper arc consistent on $c$ for $x_i$ iff all values in $D(x_i)$ are consistent with $c$ in $D$, i.e. $D(x_i)\subseteq \pi_{x_i} (c\cap \pi_{X(c)} (D))$.

                The network $N$ is hyper arc consistent iff $D$ is hyper arc consistent for all variables in $X$ on all constraints in $C$.

                The network $N$ is arc inconsistent iff $\emptyset$ is the only domain tighter than $D$ which is hyper arc consistent for all variables on all constraints.
            \end{definition}

            For corresponding network of a Sudoku problem, it is HAC if for all the units, there is at lease one permutation taking values from the corresponding domains satisfying the AllDifferent constraint inside the unit.

        \subsection {Using Solvers}
            Sudoku can be viewed as a CSP $\mathcal P=\langle X, D, C\rangle$ where $X=\langle x_{11}, x_{12}, \cdots, x_{21}, \cdots, x_{99}\rangle$ is consisted of 81 variables corresponding to 81 squares, $D=(\{1..9\}\cap A(x_{11}))\times\cdots\times(\{1..9\}\cap A(x_{99}))$, where $A(x_{ij})$ is the set of preassigned values for variable $x_{ij}$,  $C=\{c(Y)|\ Y \text{ is the set of variables in a unit}\}$ and $c(Y)$ is AllDifferent constraint on the set of variables $Y$ which means $\forall x_i,x_j\in Y,
            x_i\neq x_j$

            Ortools is a library with C, Python interface to solve operational research problems including constraint programming problems. The previous formalization of Sudoku problem as a CSP makes it possible to solve the Sudoku problem with the Constraint Programming solvers in Ortools. The Python code is as follows.

            \begin{lstlisting}[frame=single,language=python]
from ortools.sat.python import cp_model
def solve(values):
    # initialize constraint programming model
    model = cp_model.CpModel()

    # initialize variable domains
    x=[[var_from_domain(model, 'x'+str(i+1)+str(j+1), range(1,10)) for j in range(9)] for i in range(9)]


    # shrink domain w.r.t. the assignment in the grid
    for k,v in values.items():
        x[k[0]][k[1]] = var_from_domain(model, 'x'+str(k[0]+1)+str(k[1]+1), [v])

    # initialize a list of units, every square in a unit are all diff
    unitlist = []
    # column units
    unitlist += [[x[i][j] for i in range(9)] for j in range(9)]
    # row units
    unitlist += [[x[i][j] for j in range(9)] for i in range(9)]
    # box units
    unitlist += [[x[i//3*3+j//3][i%3*3+j%3] for j in range(9)] for i in range(9)]

    # add all diff constraint to every unit
    for i in unitlist:
        model.AddAllDifferent(i)

    # initialize a solver
    solver=cp_model.CpSolver()

    # solve the CSP
    status = solver.Solve(model)

    # get the solution
    solution = [[solver.Value(x[i][j]) for j in range(9)] for i in range(9)]

    # print the solution
    print('\n'.join([''.join(['{:2}'.format(item) for item in row])
          for row in solution]))
    print('\n')
            \end{lstlisting}
            This code simply solves the problem with black box solvers and is not compatible for customized optimization. So next I will explore many different formalizations and solving techniques.

        \subsection {Transformation to SAT}
            Another formalization of Sudoku problem is by treating it as a Boolean Satisfiablility Problem\cite{lardeux2008overlapping, lynce2006sudoku}. For a CSP $\langle X,D,C\rangle$, define the variables $\forall x\in X, \forall d\in D(x), \exists\mathcal V_{x,d}\in \{T,F\}$ for the corresponding SAT. Exactly one value is taken for each variable $x$, so we have $\land_{x\in X}\vee_{d\in D(x)} \mathcal V_{x,d}$ and $\land_{x\in X}\land_{d_1,d_2\in
            D(x),d_1\neq d_2}(\neg \mathcal V_{x,d_1} \vee \neg \mathcal V_{x,d_2})$.

            For a AllDifferent constraint on $n$ variables $x_i, i\in \{1..n\}$, we can decompose it into $\frac{n(n-1)}{2}$ pairwise disequality constraints $\land_{x_i,x_j\in V, i\neq j} x_i\neq x_j$.

            Although I didn't perform the reduction from pairwise disequalities to properties in the paper by myself, I found the properties in \cite{lardeux2008overlapping} useful when implementing my constraint propagation algorithm, so I persist with the notations in the paper\cite{lardeux2008overlapping}.

            $$[Om] \quad \frac{<C \wedge Alldiff(V), D>| V^{\prime} \subset V \wedge D_{V^{\prime}}=\left\{d_{1}, \ldots, d_{m}\right\}}{<C \wedge Alldiff(V), D^{\prime}>| d_{1}, \ldots, d_{m} \notin D_{V \setminus V^{\prime}}}$$

            $$[OIm] \quad \frac{<C \wedge_{i=1}^{m} Alldiff(V_{i}), D>| d \in D_{V} \wedge d \notin D_{V_{1} \setminus V}}{<C \wedge_{i=1}^{m} Alldiff(V_{i}), D^{\prime}>| d \notin \bigcup_{i=1}^{m} D_{V_{i} \backslash V}}$$



\chapter {Constraint Propagation}


    \section {The Constraint AllDifferent}

        \subsection {HAC and Binary Decomposition}
            
            The AllDifferent constraint is a global constraint, and can be decomposed into many binary constraints, where the binary decomposition is stated as follows.

            \begin{definition}
                (Binary decomposition\cite{van2001alldifferent}) Let C be a constraint on the variables $x_1,\cdots,x_n$. A binary decomposition of C is a minimal set of binary constraints $C_{dec}=\{C_1,\cdots,C_k\}$ on pairs of variables from $x_1, \cdots, x_n$ such that the solution set of $C$ equals the solution set of $\bigcap_{i=1}^k C_i$.
            \end{definition}
            By definition, the binary decomposition of AllDifferent constraint is $$\bigcap_{1<i<j<n} \{x_i\neq x_j\} $$

            For the binary decomposed AllDifferent constraints, simply applying reduction rule O1 and propagate constraints will make the network arc consistent. But the resulting tightened network is not as tight as the network tightened by applying HAC directly on AllDifferent constraint, which is the following theorem.

            \begin{theorem}
                Let P be CSP and $P_{dec}$ the same CSP in which all the AllDiffernt constraints are binary decomposed. Then $\Phi_{HA}(P)\preceq \Phi_{dec}(P)$.
            \end{theorem}

        
     \section {Propagators}

         \subsection{Régin Algorithm}

            If we want to tighten the network to reach HAC without binary decomposition, we can use Régin Algorithm\cite{regin1994filtering, anastasatospropagation}.

            This algorithm is based on some theorems in matching theory, so we have to represent the AllDifferent CSP as a value graph which is defined as follows.

            \begin{definition}
                (Value graph) Let $X$ be a set of variables and $D(X)$ the union of their domains. The bipartite graph $G=\langle X,D(X),E\rangle$ with $E=\{\{x,d\}|\ x\in X, d\in D(x)\}$ is called the value graph of $X$.
            \end{definition}
           
            So we can transform the hyper arc consistency of the AllDifferent constraint into the existency of a matching in $G$, which is the following theorem.

            \begin{theorem}
                (Régin\cite{regin1994filtering}) Let $X=\{x_1,\cdots,x_n\}$ be a set of variables and let $G$ be the value graph of $X$. Then $\langle d_1,\cdots,d_n\rangle\in alldifferent(x_1,\cdots,x_n)$ if and only if $M=\{\{x_1,d_1\},\cdots,\{x_n,d_n\}\}$ is a matching in $G$.
            \end{theorem}

            The next theorem is exactly the main idea of Régin algorithm, which means by finding one maximum-cardinality matching will let us know all the edges in maximum-cardinality matchings.

            \begin{theorem}
                Let G be a graph and M a maximum-cardinality matching in G. An edge e belongs to some maximum-cardinality matching in G iff $e\in M$ or e in on an even-length M-alternating path starting at an M-free vertex, or e is on an even-length M-alternating circuit.
            \end{theorem}

            So we have the HAC algorithm \ref{HAC} for AllDifferent constraints.

            \begin{algorithm}
                \caption{Search and propagate with HAC}
                \label{HAC}
                \begin{algorithmic}[1]
                    \Require Value graph of the CSP $G_g$, variable set of a unit $U\subseteq X$
                    \Ensure Pruned value graph $G_g$ if it's solvable, otherwise $F$
                    \Function{Filtering}{$G_g$, $U$}
                        \If{not $solvable(G_g)$}
                            \State \Return $F$
                        \EndIf
                        \State $G\leftarrow G_g[U\cup D(U)]$ \Comment{Get the induced subgraph to perform HAC locally}
                        \State $M\leftarrow$ \Call{HopcroftCarpMatching}($G, U$)
                        \State $A_{dx}\leftarrow \{(d_i,x_i|\ \{d_i,x_i\}\in E(G)\setminus M \}$\Comment{Arcs from value to variable}
                        \State $A_{xd}\leftarrow \{(x_i,d_i|\ \{x_i,d_i\}\in M\}$ \Comment{Arcs from variable to value}
                        \State $G_M\leftarrow DiGraph(A_{dx}\cup A_{xd})$
                        \State $S\leftarrow M$
                        \For{$i$ in \Call{StronglyConnedtedComponents}{$G_M$}}
                            \State $S \leftarrow S\cup A(G_M[i])$
                        \EndFor
                        \State $V_{free} \leftarrow V(G)\setminus V(M)$
                        \If{$V_{free}\neq \emptyset$}
                            \For{$i$ in \Call{BFSNext}{$G_M,V_{free}$}} \Comment{$i$ is the set of next arcs}
                                \For{$j$ in $i$}
                                    \State $S\leftarrow S\cup j$
                                \EndFor
                            \EndFor
                        \EndIf
                        \State $G_g\leftarrow G_g \setminus(E(G)\setminus S)$
                    \EndFunction
                \end{algorithmic}
            \end{algorithm}

            I implemented this algorithm in Python, by calling this filtering function recursively with backtracking can solve the sudoku within a few searches. The code is as follows.
            \begin{lstlisting}[frame=single, language=python]
def filtering(GG, u):
    # filter GG with constraint u and return if GG is still satisfiable
    v = set()
    for i in unitlist[u]:
        if not set(GG[i]):
            return False
        v |= set(GG[i])
    G = GG.subgraph(unitlist[u]+list(v))
    max_matching=hopcroft_karp_matching(G, unitlist[u])
    max_matching=[(k,v) for k,v in max_matching.items()]
    v2x = [p if len(p[1])==2 else p[::-1] for p in list(map(tuple, set(map(frozenset,G.edges)) - set(map(frozenset,max_matching))))]
    x2v = [p if len(p[0])==2 else p[::-1] for p in max_matching]
    assert {i for p in G.edges for i in p}==set(G.nodes)
    GM=nx.DiGraph(v2x+x2v)
    used=set(map(frozenset,max_matching))
    for i in nx.strongly_connected_components(GM):
        used |= set(map(frozenset,GM.subgraph(i).edges))
    m_free = list(set(G.nodes)-{i for p in max_matching for i in p })
    if m_free:
        for i in nx.bfs_successors(GM,m_free[0]):
            for j in i[1]:
                used.add(frozenset({i[0],j}))
    unused = list(map(tuple, set(map(frozenset,G.edges)) - used))
    GG.remove_edges_from(unused)

    affected_units = set()
    for e in unused:
        affected_units |= units[e[0] if len(e[0])==2 else e[1]]
    for unit in list(affected_units - {u}):
        if not filtering(GG, unit):
            return False
    return True

def solved(G):
    # return if G is solved
    for i in range(9):
        for j in range(9):
            if len(G[(i,j)])!=1:
                return False
    return True

def assign(G, var, value):
    # assign "value" to variable "var"
    G.remove_edges_from([e for e in G.edges(var) if value not in e])

def min_domain_variable(G):
    # return the varible with the minimum size of domain
    min_domain=10
    for i in range(9):
        for j in range(9):
            if len(G[(i,j)])>1 and len(G[(i,j)])<min_domain:
                min_domain = len(G[(i,j)])
                min_var = (i,j)
    return min_var

def search_propagate(G, affected):
    # filtering with affected constraints and search by assigning value and propagate, return solution if exists, otherwise return False
    for i in affected:
        if not filtering(G,i):
            return False
    if solved(G):
        return G
    var = min_domain_variable(G)
    values = set(G[var])
    while values & set(G[var]):
        values &= set(G[var])
        d = values.pop()
        G_cp = G.copy()
        assign(G_cp, var, d)
        affected = units[var]
        sol = search_propagate(G_cp, affected)
        if sol:
            return sol
        else:
            if (var,d) in G.edges:
                G.remove_edge(var,d)
                for i in units[var]:
                    if not filtering(G, i):
                        return False
    return False;
            \end{lstlisting}
            

            As stated in \cite{simonis2005sudoku}, the constraints interact in multiple ways and tightening with local consistency on each AllDifferent constraint alone cannot exploit all these relations and eliminate all the nogoods, i.e. cannot be backtrack free. So we have to balance between heavy constraint reasoning and propagation and brute force searching in case there is too much time speet on reasoning.

            In the experiment of previous HAC algorithm which performs Régin Algorithm on each unit separately and propagate, the time cost can be even more than simply using $GAC$ on decomposed binary constraints. Because the search
            space prunned by constraint reasoning is not overwhelmingly large for some specific cases and the time spent on constraint reasoning is too much.
        \subsection{Reduction Rules}

            Inspired by the reduction rules found in \cite{lardeux2008managing,lardeux2008overlapping}, I implemented the algorithm \ref{eliminate} to perform constraint propagation with O1, O8 and OI2 redunction rules. This algorithm eliminates already assigned values from all the domains of its peers, which is O1 redunction rule. And if a value only appears once in the domains of a unit, assign the sqaure with the value, which is O8 reduction rule. For higher order consistency, when two units overlap
            and the union of the domains of the intersected variables has no intersection with all the domains of other squares in a unit, eliminate the union from all the domains of the other unit, which is OI2.

            \begin{algorithm}
                \caption{Eliminate with propagation}
                \label{eliminate}
                \begin{algorithmic}[1]
                    \Require Map $\mathcal V:X\times D(X) \to \{T,F\}$, $x_i\in X$, $d_j\in D(x_i)$
                    \Ensure pruned map $\mathcal V$ if solvable, otherwise $F$.
                    \Function{Eliminate}{$\mathcal V$, $x_i$, $d_j$}
                        \State $\mathcal V(x_i, d_j)=F$
                        \If{$\nexists d', s.t. \mathcal V(x_i, d')=T$}
                            \State \Return $F$
                        \EndIf
                        \If{Exactly one $d', s.t. \mathcal V(x_i, d')=T$}
                            \For{$x_p\in Peers(x_i)$}
                                \State $\mathcal V\leftarrow$ \Call{Eliminate}{$\mathcal V, x_p, d'$} \Comment{O1 propagator}
                                \If{$\mathcal V= F$}
                                    \State \Return $F$
                                \EndIf
                            \EndFor
                        \EndIf
                        \For{Unit $U\in Units(x_i), U\subseteq X$}
                            \If{Exactly one variable $x_u$ in $U\setminus \{x_i\}$ can be assigned with $d_j$} 
                                \State Assign $x_u$ with value $d_j$ and propagate \Comment{O8 propagator}
                                \If{Assign failed}
                                    \State \Return $F$
                                \EndIf
                            \EndIf
                        \EndFor
                        \For{Box unit $U_1\in Units(x_i)$ and another unit $U_2\neq U_1, U_2\in Units(x_i)$}
                            \State Get locked candidate set $L\leftarrow D(U_1\cap U_2)\setminus D(U_1\setminus U_2)$
                            \For{$x_u$ in $U_2\setminus U_1$ and $d_u$ in $L$}
                                \State $\mathcal V\leftarrow$ \Call{Eliminate}{$\mathcal V, x_u, d_u$} \Comment{Eliminate all the values in locked candidate set from $U_2\setminus U_1$}
                                \If{$\mathcal V=F$}
                                    \State \Return $F$
                                \EndIf
                            \EndFor
                        \EndFor

                    \EndFunction
                \end{algorithmic}
            \end{algorithm}

\chapter {Tree search}

    \section {Searching Heuristics}
        \subsection {Minimum Remaining Values}
            According to \cite{golomb1965backtrack}, there is a MRV heuristic, which the only heuristic used in my code because it is easy to implement and efficient enough.
            \begin{algorithm}
                \caption{SearchMRV}
                \label{MRV}
                \begin{algorithmic}[1]
                    \Require Map $\mathcal V:X\times D(X) \to \{T,F\}$, $\mathcal V(x_i,d_j)=T$ if and only if there is a value $d_j$ in the domain $D(x_i)$ of variable $x_i$, otherwise $\mathcal V(x_i,d_j)=F$. 
                    \Ensure $\mathcal V'$ if it is a solution, if no solution is found, $F$.
                    \Function{SearchMRV}{$\mathcal V$}
                        \If{$solved(\mathcal V)$}
                            \State \Return $\mathcal V$
                        \EndIf
                        \State Get the variable $x_{min}$ with the minimum cardinality of domain $|D(x_{min})|$.
                        \For{$d$ in $D(x_{min})$}
                            \State assign $d$ to variable $x_{min}$ and propagate constraints in $\mathcal V$
                            \If{assigning is successful}
                                \State $\mathcal V'\leftarrow$ \Call{SearchMRV}{$\mathcal V$}
                                \If{$\mathcal V'\neq F$}
                                    \State \Return $\mathcal V'$
                                \EndIf
                            \EndIf
                        \EndFor
                        \State \Return $F$
                    \EndFunction
                \end{algorithmic}
            \end{algorithm}

        \subsection{Forward Checking}
            Forward checking keeps arc consistency for a constraint $c(V)$ if only one variable $x_i\in V$ is not instantiated during the process of tree search\cite{golomb1965backtrack}. So forward checking for the binary decomposed AllDifferent constraint of Sudoku problem is equivalent to the process of eliminating the digit from rows, columns and boxes whenever a square is assigned a digit, which gives the algorithm \ref{FC}
            \begin{algorithm}
                \caption{Forward Checking}
                \label{FC}
                \begin{algorithmic}[1]
                    \Require Map $\mathcal V:X\times D(X) \to \{T,F\}$, $x_i\in X$, $d_j\in D(x_i)$
                    \Ensure pruned map $\mathcal V$ if solvable, otherwise $F$.
                    \Function{ForwardChecking}{$\mathcal V$, $x_i$, $d_j$}
                        \For{$\forall c_{ik}=\{ x_i\neq x_k\}= C(x_i,x_k)=C$ }
                            \State $\mathcal V(x_k,d_j)=F$ \Comment{Eliminate $d_j$ from all the variables $x_k$ with constraint $c_{ik}$}
                        \EndFor
                        \If{$\mathcal V$ if $\forall x_i\in X,\exists d_j\in D(x_i), \mathcal V(x_i,d_j)=T$}
                            \State \Return $\mathcal V$
                        \Else
                            \State \Return $F$
                        \EndIf
                    \EndFunction
                \end{algorithmic}
            \end{algorithm}

        \subsection{Conflict-Directed Backjumping}
            \begin{definition} (nogood) A nogood is a set of assignments and branching constraints that is not consistent with any solution\cite{rossi2006handbook}.
            \end{definition}

            Conflict Directed Backjumping(CBJ) is a technique that utilizes the predefined nogood which can help ruling out branches of search tree to perform several back tracking steps all at once\cite{prosser1993hybrid}.


            For Sudoku problem, if $x_1,\cdots, x_9\in X$ are in the same unit, $|D(x_1)|>1$ and $D(x_1)\cap D(x_i)=\emptyset, x\in\{2,\cdots,9\}$, then the problem is not solvable. I use this nogood in my code to perform CBJ, which is algorithm \ref{backjumping}.

             \begin{algorithm}
                \caption{Conflict Directed Backjumping}
                \label{backjumping}
                \begin{algorithmic}[1]
                    \Require Map $\mathcal V:X\times D(X) \to \{T,F\}$
                    \Ensure $\mathcal V'$ if it is a solution, if no solution is found, $F$.
                    \Function{SearchCDB}{$\mathcal V$}
                        \If{$solved(\mathcal V)$}
                            \State \Return $\mathcal V$
                        \EndIf
                        \State Get the variable $x_{min}$ with the minimum cardinality of domain $|D(x_{min})|$.
                        \For{$d$ in $D(x_{min})$}
                            \For{A set of variables $Y$ having AllDifferent constraint with $x_{min}$(For Sudoku problem, if $U$ is a unit, then $Y=U\setminus \{x_{min}\}$)}
                                \If{$D(x_{min})\cap D(Y)=\emptyset$}
                                    \State \Return F  \Comment{Use nogood to backjump}
                                \EndIf
                            \EndFor
                            \State assign $d$ to variable $x_{min}$ and propagate constraints in $\mathcal V$
                            \If{assigning is successful}
                                \State $\mathcal V'\leftarrow$ \Call{SearchCDB}{$\mathcal V$}
                                \If{$\mathcal V'\neq F$}
                                    \State \Return $\mathcal V'$
                                \EndIf
                            \EndIf
                        \EndFor
                        \State \Return $F$
                    \EndFunction
                \end{algorithmic}
            \end{algorithm}
    \section {Comparisons}
        Although AllDifferent constraint HAC algorithm will eliminate more search space, it is so slow that constraint reasoning might take more time than searching and backtracking. For all the other methods, I implemented them in one pure C program. I noticed that by adding OI2 reduction rule (which is only path consistency propagator), the performance of the algorithm even degenerate. I analyzed the algorithm and discovered that almost all higher order consistencies are
        time consuming to perform, which may instead worsen the time complexity.

\bibliography{../report_bib}{}
\bibliographystyle{plain}
\end{document}
